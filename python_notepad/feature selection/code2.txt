#!pip3 install mlxtend

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=4)

rom mlxtend.feature_selection import SequentialFeatureSelector as SFS

#Parameters to understand

sfs = SFS(knn, 
          k_features=3, 
          forward=False, 
          floating=True, 
          scoring='accuracy',
          cv=3,
          n_jobs=-1)
feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')
sfs = sfs.fit(X, y,custom_feature_names=feature_names)

import pandas as pd
pd.DataFrame.from_dict(sfs.get_metric_dict()).T

###Check the page
#http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/?msclkid=bd438f62ae9a11ecb61982e554e0e047

####RECURSIVE FEATURE ELIMINATION####
Feature ranking with recursive feature elimination and cross-validated selection of the best number of features

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Loading pre-defined Boston Dataset
boston_dataset = datasets.load_boston()

# Load the dataset into Pandas Dataframe
boston_pd = pd.DataFrame(boston_dataset.data)
boston_pd.columns = boston_dataset.feature_names
boston_pd_target = np.asarray(boston_dataset.target)
boston_pd['Price'] = pd.Series(boston_pd_target)
 
# input
X = boston_pd.iloc[:, :-1]
 
#output
Y = boston_pd.iloc[:, -1]

x_train, x_test, y_train, y_test = train_test_split(
    boston_pd.iloc[:, :-1], boston_pd.iloc[:, -1],
    test_size = 0.20)

 
print(boston_pd.head())

X, y = x_train, y_train

from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV


lin_reg = LinearRegression()

rfe_mod = RFECV(lin_reg,cv=2) 
myvalues=rfe_mod.fit(X,y) 
myvalues.support_
myvalues.ranking_ 

print("Num Features: %s" % (myvalues.n_features_))
print("Selected Features: %s" % (myvalues.support_))
print("Feature Ranking: %s" % (myvalues.ranking_))  

####Using Models to select features - Embedded Methods

from sklearn.linear_model import Lasso
 

lasso = Lasso(alpha = 1)
lasso.fit(x_train, y_train)
y_pred1 = lasso.predict(x_test)
 

mean_squared_error = np.mean((y_pred1 - y_test)**2)
print("MSE on test set", mean_squared_error)
lasso_coeff = pd.DataFrame()
lasso_coeff["Columns"] = x_train.columns
lasso_coeff['Coefficient Estimate'] = pd.Series(lasso.coef_)
 
print(lasso_coeff)


######Using Random Forest to Select features########

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = 100,random_state=42)
rf.fit(x_train, y_train)
importances = rf.feature_importances_

forest_importances = pd.Series(importances, index=x_train.columns)

fig, ax = plt.subplots()
forest_importances.plot.bar()
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()



#this is also used to check feature importance in classification as will as regression problem###
from sklearn.ensemble import ExtraTreesClassifier #classification problem

from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()
model.fit(x,y)

print(model.feature_importances_)

#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=x.columns)
feat_importances.nlargest(5).plot(kind='barh')
plt.show()    #shows top 5 important features


