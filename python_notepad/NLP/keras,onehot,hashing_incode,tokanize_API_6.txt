     ####### keras in NLP #########
 Keras he yek API(application Programming Interface) aahe aahe jeki to software kiva application la yekamenamadhe communicate karnyasathi invirment create kart. ithun mage je appn baghitl te aapn ithe keras madhe karun baghu.

#Data Clearning with Kears ##Split Words with text to word sequence A good first step when working with text is to split it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the text to word sequence() function that you can use to split text into a list of words. Bydefault, this function automatically does 3 things:

 Splits words by space.

 Filters out punctuation.

 Converts text to lowercase (lower=True).
#varcha tin process outomatic hotat keras madhe word space nusar saperate honar puctuation janar aahe sagle word outomatic lower case madhe jatat.

from keras.preprocessing.text import text_to_word_sequence  #import kel keras madhun
# define the document
text = 'The quick brown fox jumped over the lazy dog.'
# tokenize the document
result = text_to_word_sequence(text)   #text war text_to_word_sequence aaply kela
print(result)

  ######## one hot incoder ##########
#Encoding with one hot Keras provides the one hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one hot encoding of the document, which is not the case. Instead, the function is a wrapper for the hashing trick() function. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values. As with the text to word sequence() function in the previous section, the one hot() function will make the text lower case, filter out punctuation, and split words based on white space.
(he word la token madhe convert karun tya token la one hot incoding apply karun number madhe convert kart jith repeat word asel tith same value value jate)

from keras.preprocessing.text import text_to_word_sequence
# define the document
text = 'The quick brown fox jumped over the lazy dog.'
# estimate the size of the vocabulary
words = set(text_to_word_sequence(text))
vocab_size = len(words)       #size baghitli aapla token chi
print(vocab_size)           #size print keli

from keras.preprocessing.text import one_hot
from keras.preprocessing.text import text_to_word_sequence
# define the document
text = 'The quick brown fox jumped over the lazy dog.'
# estimate the size of the vocabulary
words = set(text_to_word_sequence(text))
vocab_size = len(words)
print(vocab_size)
# integer encode the document
result = one_hot(text, round(vocab_size*1.3))  #one_hot madhe import karun text sentene dil aani sixe of the token sangitl mg aata he tanusar answer deil.
print(result)

    ######## Hash incoding ##########
he jas ki mag baghitl hot memory consumption he kami pramanat ghet. baki procedure varchagt aahe.

#Hash Encoding with hashing trick Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function. It provides more flexibility, allowing you to specify the hash function as either hash (the default) or other hash functions such as the built in md5 function or your own function. Below is an example of integer encoding a document using the md5 hash function

from keras.preprocessing.text import hashing_trick
from keras.preprocessing.text import text_to_word_sequence
# define the document
text = 'The quick brown fox jumped over the lazy dog.'
# estimate the size of the vocabulary
words = set(text_to_word_sequence(text))
vocab_size = len(words)
print(vocab_size)
# integer encode the document
result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')  #ithe md5 function vapr karto varchagtch output det pn last value ithe consider hote varchat last value consider hot navti.
print(result)


  ######### tokenizer API ########
aapn he multiple time use karu shakto aani large dataset la yacha vaper hoto.

#Tokenizer API Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects. Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.

from keras.preprocessing.text import Tokenizer
# define 5 documents
docs = ['Well done!',
'Good work',
'Great effort',
'nice work',
'Excellent!']
# create the tokenizer
t = Tokenizer()
# fit the tokenizer on the documents
t.fit_on_texts(docs)                   # tokanizer fit kela

tokanizer fit kelaver kalcha gosti kahi automatically hotat

Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:

 word counts: A dictionary of words and their counts.

 word docs: An integer count of the total number of documents that were used to fit the Tokenizer.

 word index: A dictionary of words and their uniquely assigned integers.

 document count: A dictionary of words and how many documents each appeared in.

# summarize what was learned
print(t.word_counts)
print(t.document_count)
print(t.word_index)
print(t.word_docs)     #ya char value atomatic sangt he

yekda tokanizer fit zala tr aapn traing aani testing doni la yacha vaper karu shakto


Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets. The texts to matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:

 binary: Whether or not each word is present in the document. This is the default.

 count: The count of each word in the document.

 tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.

 freq: The frequency of each word as a ratio of words within each document.

##matrix pn create karu shakto

from keras.preprocessing.text import Tokenizer
# define 5 documents
docs = ['Well done!',
'Good work',
'Great effort',
'nice work',
'Excellent!']
# create the tokenizer
t = Tokenizer()
# fit the tokenizer on the documents
t.fit_on_texts(docs)
# summarize what was learned
print(t.word_counts)
print(t.document_count)
print(t.word_index)
print(t.word_docs)
# integer encode documents
encoded_docs = t.texts_to_matrix(docs, mode='count')  #matrix madhe convert karun baghitl
print(encoded_docs)