-        #####  Supervised machine learning - Regression Problem  ######    dataset.nunique()  #check no of unique of data

   ## Simple Linear Regression ##
  
   df1= pd.read_csv('Salary_Data.csv')
   plt.plot(df1["YearsExperience"], df1["Salary"])  ## simply line drow to check the correlation of data
        ## dependant variable

   from sklearn.model_selection import train_test_split
   x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)  

   (x_train.shape, y_train.shape, x_test.shape, y_test.shape)   # check the shape after splitting

   from sklearn.linear_model import LinearRegression
   reg = LinearRegression()
   reg.fit(x_train, y_train)
   reg
  reg.score(x_test,y_test)  ##how will train the model accurately 

  y_pred= reg.predict(x_test)
  print (y_pred)                  ## predict data on test

  from sklearn.metrics import r2_score
  print ('Coefficient', reg.coef_)
  print ('Intercept', reg.intercept_)        ##y=mx+c

  r2_score(y_test,y_pred)  ##accuracy of testing data


      x_final = pd.DataFrame(x,columns= ['Experience'])
      y_final = pd.DataFrame(y,columns= ['Salary'])
      y_pred_final = pd.DataFrame(y_pred,columns= ['Salary Prediction'])              #### compare predictions
      result = pd.concat([x_final,y_final,y_pred_final], axis =1)
      print (result)
      result.to_excel("Simple Regression.xlsx")


   plt.scatter(x_train,y_train)
   plt.plot (x_train,reg.predict(x_train),'red' )      ##traning dataset check prediction what is the point will differ for actual line
   plt.xlabel('x-axis')
   plt.ylabel('y-axis')
   plt.title('LinearRigration')
   plt.show()
 

 plt.scatter(df['cgpa'],df['package'])       ##scatter plot
plt.xlabel('CGPA')
plt.ylabel('Package(in lpa)')

   print(lr.predict(X_test[0]))    ##predict one value of data

print("R2 score",r2_score(y_test,y_pred))
r2 = r2_score(y_test,y_pred)                #formula for R^2

 1 - ((1-r2)*(40-1)/(40-1-2))               #adj R^2



     ### Gradian disent  ###

    ##it is use for drow the best fit line with low cost function

   reg = LinearRegression()
   reg.fit(X,y)
   reg.coef_
   reg.intercept_

  plt.scatter(X,y)
  plt.plot(X,reg.predict(X),color='red')

  y_pred = ((78.35 * X) + 0)
  y_pred

 plt.scatter(X,y)
 plt.plot(X,reg.predict(X),color='red',label='OLS')
 plt.plot(X,y_pred,color='#00a65a',label='b = 0')
 plt.legend()
 plt.show()
 
b = 0
m = 78.35
lr = 0.1

epochs = 10

for i in range(epochs):
  loss_slope = -2 * np.sum(y - m*X.ravel() - b)
  b = b - (lr * loss_slope)

  y_pred = m * X + b

  plt.plot(X,y_pred)                                   ##using that looping calculate the best fit line

plt.scatter(X,y)



  ###### Multiple linear regression  ####

 Multiple linear regression is same as simple linear regression but change is simply that use more than one independant variable

 other method is OLS to fit regression model

x_new=x_new.astype('float64')
import statsmodels.api as sm
reg_ols = sm.OLS (endog = y, exog = x_new)
reg_ols = reg_ols.fit()
print (reg_ols.summary())

from sklearn.compose import ColumnTransformer                                                      #onehotincoding through col transformer
from sklearn. preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers= [('en',OneHotEncoder(), [3] )],remainder='passthrough')
x1 = ct.fit_transform(x)
x1

         ##########  Polynomial Regression- Data has an curvy nature or nonlinear natuture the we use that regreation (y=a + bx + cx^2 + dx^3 + ------mx^n) ###########

os.chdir('C:\\Users\\Hp\\Desktop\\Pyt_language\\machine-learning\\Height_Weight_Dataset')
df = pd.read_csv('Height_Weight_Dataset.csv')
df.head(50)                                         ## data

plt.plot(x,y)
plt.xlabel('Age')
plt.ylabel('Height')
plt.plot()                         ##create x y variable and plot the data and check line though behaviour of the data

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 50)   ##train test split

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train,y_train)                               ##fit the line of regression

y_pred = lr.predict(x)
y_pred

plt.plot (x,y)
plt.plot (x,y_pred)

from sklearn.preprocessing import PolynomialFeatures
polynom = PolynomialFeatures(degree = 2)
x_polynom = polynom.fit_transform(x)
pd.DataFrame(x_polynom).head(50)                     ## fit polynomial with degree 2

from sklearn.model_selection import train_test_split
xpoly_train,xpoly_test,ypoly_train,ypoly_test = train_test_split(x_polynom,y,test_size =0.3,random_state = 0)

PolyReg = LinearRegression()
PolyReg.fit(xpoly_train,ypoly_train)

ppred= PolyReg.predict(x_polynom)
ppred

plt.plot(x,y)
plt.plot(x,y_pred)
plt.plot(x,ppred, color = 'r')  #plot the regression line

r2_score(y,ppred)



               ########### L1 and L2 regularization  ###########

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(train_X, train_y)

reg.score(test_X, test_y)        #accuracy 13%       here traning pace accuracy is high but testing phase is low so this is problem of overfitting
reg.score(train_X, train_y)      #accuracy 69%
                                                 ##below models reduce problem of the overfiting of the model | and also known as feature selection technique

        ##### L2(Ridge  regularization) - this use to loss + alpha ||w(slop)||^2  use reduce complexity coefficient of variable minimize but not zero ######

from sklearn import linear_model
lasso_reg = linear_model.Lasso(alpha=500, max_iter=100, tol=0.1)
lasso_reg.fit(train_X, train_y) 

lasso_reg.score(test_X, test_y)
lasso_reg.score(train_X, train_y)    both have same accuracy


   ##### L1(LASSO regularization) - this use to loss + alpha ||w||  use reduce complexity | coefficient of variable zero if they have low importance ######

from sklearn.linear_model import Ridge
ridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)
ridge_reg.fit(train_X, train_y)

ridge_reg.score(test_X, test_y)
ridge_reg.score(train_X, train_y)    both have same accuracy


                         ###################  SVM regression  ########################

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state =50)
print('shape of x_train =',x_train.shape)
print('shape of x_test =',x_test.shape)
print('shape of y_train =',y_train.shape)
print('shape of y_test=',y_test.shape)                      ##train test split

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(x_train)
x_train = sc.transform(x_train)
x_test = sc.transform(x_test)

x_train                                                  ### using  standerd scaler convert data into dandered normal form

from sklearn.svm import SVR
svr_rbf = SVR(kernel= 'rbf')
svr_rbf.fit(x_train,y_train)
svr_rbf.score(x_test,y_test)         ##unsing rbf karnel fit svm model

svr_linear = SVR(kernel='linear')
svr_linear.fit(x_train,y_train)
svr_linear.score(x_test,y_test)       ##unsing linear karnel fit svm model

svr_poly = SVR(kernel='poly',degree = 3)
svr_poly.fit(x_train,y_train)
svr_poly.score(x_test,y_test)              ##unsing polynomial karnel fit svm model  

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test,y_pred)
rmse = np.sqrt(mse)
print('MSE =',mse)
print('RMSE =',rmse)                         #find MSE and RMSE



    ################ Decision Tree regression ################

        
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 50)   ##train test split

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train,y_train)                               ##fit the line of regression

y_pred = lr.predict(x)
y_pred                                                ##fit simple regression and check accuracy
r2_score (y_test,y_pred)


from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
dt.fit(x_train,y_train)                           ##fit the line of regression
dtpred = dt.predict (x)
dtpredr2_score (y,dtpred)



              ############################ KNN Regression ##############################

from sklearn import neighbors
from sklearn.metrics import mean_squared_error 
from math import sqrt
import matplotlib.pyplot as plt 

rmse_val =[]
for K in range(30):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K,matrix='minkowski',p=2)   # use icludian,mann hatten and minkiwoski distance minkiwoski is combination of 
    model.fit(x_train, y_train)  #fit the model                                     #man hatten and icludian if p=2 use man hatten distance also use hamming distance
    pred=model.predict(x_test) #make prediction on test set
    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse
    rmse_val.append(error) #store rmse values
    print('RMSE value for k= ' , K , 'is:', error)                        #fit line with KNN for using loop to select best value of k is to minimum RMSE

plt.plot(range(1,21),rmse_val)                             ## Plot on graph


     ############# multi-output regression variable ###########
     
 if your dataset have the multiple output means more than two dependant or responce variable so you can use that model for the prediction
 
 from sklearn.multioutput import MultiOutputRegressor
 
 
